{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dd4653-dd3b-430b-b3d3-f059a249c3f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "This Jupyter notebook contains codes to implement the numerical experiments in [Approximating persistent homology for large datasets](https://arxiv.org/abs/2204.09155), and is organized into four sections:\n",
    "\n",
    "- Verify convergence rate on torus\n",
    "- Persistence measure v.s. Fréchet mean\n",
    "- PH approximation on massive data\n",
    "- Shape clustering\n",
    "\n",
    "The computation of persistent homology is implemented using `gudhi` (https://gudhi.inria.fr), and the computation of optimal transport is implemented using `pot` (https://pythonot.github.io).\n",
    "\n",
    "**Caveat**: The codes in general are not fast, and some can take up to 7 hours. Therefore we do not recommend the  option `Run all cells`. Instead, please run cells section by section. \n",
    "\n",
    "Author: Yueqi Cao\n",
    "\n",
    "Contact: y.cao21@imperial.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a71b54b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T12:41:38.822444Z",
     "start_time": "2021-11-21T12:41:38.807467Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import numpy as np\n",
    "np.random.seed(20211121) # set random seed for reproducible experiments\n",
    "import ApproxPH\n",
    "import matplotlib.pyplot as plt\n",
    "from gudhi.wasserstein.barycenter import lagrangian_barycenter as bary\n",
    "\n",
    "# function to compute mean persistence measure and Frechet mean\n",
    "def compute_mean(original_set, nb_subs, nb_sub_points, max_edge_length, min_persistence, scenario):\n",
    "    subs = ApproxPH.get_subsample(original_set, nb_sub_points, nb_subs)\n",
    "    # list of PDs\n",
    "    diags = []\n",
    "    for points in subs:\n",
    "        diag = ApproxPH.get_PD(points, max_edge_length=max_edge_length, min_persistence=min_persistence)\n",
    "        diag[np.isinf(diag)] = max_edge_length\n",
    "        diags.append(diag)\n",
    "        \n",
    "    if scenario == 'mpm':\n",
    "        # compute mean persistence measure\n",
    "        sub_pers = np.array([[0,0]])\n",
    "        for diag in diags:\n",
    "            sub_pers = np.append(sub_pers, diag, axis=0)\n",
    "        unit_mass = 1/nb_subs\n",
    "        mean_mesr, mean_mesr_vis = ApproxPH.diag_to_mesr(sub_pers, unit_mass)\n",
    "        return mean_mesr, mean_mesr_vis\n",
    "    \n",
    "    if scenario == 'fm':\n",
    "        # compute Frechet mean\n",
    "        wmean, log = bary(diags, init=0, verbose=True)\n",
    "        return wmean\n",
    "    \n",
    "    if scenario == 'both':\n",
    "        # compute both mean persistence measure and Frechet mean\n",
    "        wmean, log = bary(diags, init=0, verbose=True)\n",
    "        sub_pers = np.array([[0,0]])\n",
    "        for diag in diags:\n",
    "            sub_pers = np.append(sub_pers, diag, axis=0)\n",
    "        unit_mass = 1/nb_subs\n",
    "        mean_mesr, mean_mesr_vis = ApproxPH.diag_to_mesr(sub_pers, unit_mass)\n",
    "        return mean_mesr, mean_mesr_vis, wmean    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff29826",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Verify convergence rate on torus\n",
    "\n",
    "In this section, we test the convergence rate of mean persistence measure on a synthetic dataset sampled from a 2-dimensional torus. \n",
    "\n",
    "Let $X_T$ be the large dataset from the torus, and $\\pi$ be the discrete measure on $X_T$. When $X_T$ is uniformly sampled from the 2-dim torus, $\\pi$ is assumed to satistfy the $(a,2,r_0)$-standard measure. In a nutshell, any ball of radius $r>r_0$ will have points proportional to $r^2$. Let $S_n^{(1)},\\ldots,S_n^{(B)}$ be $B$ i.i.d. subsample sets from $X_T$, each consisting of $n$ points. Then we have the following estimation of approximation error\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\mathrm{OT}_p^p(\\bar{D},D[X_T])]\\leq O(B^{-1/2})+O(1)+O(n^{-(p/2-1)}),\\quad p>2\n",
    "$$\n",
    "\n",
    "where $\\mathrm{OT}_p$ is the optimal partial transport distance, and $\\bar{D}$ is the mean persistence measure. If we take $B=O(n)$ then the error bound is $O(1)+O(n^{-\\min\\{1/2,p/2-1\\}})$. Thus when $p=3$ or $p=8$ ideally we shall see a rate of -0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edcc9cdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T11:03:34.656866Z",
     "start_time": "2021-11-21T10:59:53.495462Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate true set and compute persistent homology\n",
    "# this step takes about 4 mins.\n",
    "X_torus = ApproxPH.sample_torus(50000, 0.8, 0.3)\n",
    "np.save('outputs/true-torus-points.npy', X_torus)\n",
    "diag_torus = ApproxPH.get_PD(X_torus, max_edge_length=0.9)\n",
    "np.save('outputs/true-torus-diagram.npy', diag_torus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8edaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T11:12:00.563607Z",
     "start_time": "2021-11-21T11:11:58.322424Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the true persistence diagram\n",
    "# use the following command if you have true-torus-diagram prepared \n",
    "# diag_torus = np.load('outputs/true-torus-diagram.npy')\n",
    "ApproxPH.plot_diag(diag_torus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ee5a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T20:21:38.376485Z",
     "start_time": "2021-11-21T12:41:45.544670Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract subsamples from the true set\n",
    "# use the following command if you have true-torus-points prepared\n",
    "#X_torus = np.load('outputs/true-torus-points.npy')\n",
    "# it takes about 7 hours to run 15 simulations\n",
    "nb_simulates = 15\n",
    "for i in range(nb_simulates): \n",
    "    mean_mesr, mean_mesr_vis = compute_mean(original_set = X_torus,\n",
    "                                            nb_subs = 20*(i+2),\n",
    "                                            nb_sub_points = 200*(i+2),\n",
    "                                            max_edge_length = 0.9,\n",
    "                                            min_persistence = 0.01,\n",
    "                                            scenario = 'mpm'\n",
    "                                           )\n",
    "    np.save('outputs/mean_mesr_nb%d.npy' %(i), mean_mesr)\n",
    "    print('mean persistence measure for %dth simulation' %(i))\n",
    "    # use the following command to visualize the mean persistence measure\n",
    "    # mpd.plot_mesr(mean_mesr_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee66d291",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T08:48:51.641185Z",
     "start_time": "2021-11-22T08:48:48.325745Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute Wasserstein distances between mean persistence measures and true persistence diagram\n",
    "mesr_list = []\n",
    "for i in range(15):\n",
    "    mesr = np.load('outputs/mean_mesr_nb%d.npy' %(i))\n",
    "    mesr_list.append(mesr)\n",
    "\n",
    "# load the true PD\n",
    "true_PD = np.load('outputs/true-torus-diagram.npy')\n",
    "# transform the true PD to PM\n",
    "true_mesr, true_mesr_vis = ApproxPH.diag_to_mesr(true_PD, 1)\n",
    "\n",
    "# compute the Wasserstein distance\n",
    "power_index = 3\n",
    "grid = ApproxPH.mesh_gen()\n",
    "Mp = ApproxPH.dist_mat(grid, power_index)\n",
    "dist_list = []\n",
    "point_list = []\n",
    "for i in range(len(mesr_list)):\n",
    "    distance = ApproxPH.wass_dist(mesr_list[i], true_mesr, Mp)\n",
    "    point_list.append(200*(i+2))\n",
    "    dist_list += distance.tolist()\n",
    "    \n",
    "# plot fitting curve\n",
    "ApproxPH.plot_fitting_curve(point_list, dist_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6400f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-24T16:15:08.256715Z",
     "start_time": "2021-11-24T16:15:08.245704Z"
    }
   },
   "source": [
    "# Comparison with the Fréchet mean method\n",
    "\n",
    "In this section, we compare the performance of mean persistence measure and Fréchet mean on a synthetic dataset sampled from an anulus. The performance is measured by the 2-Wasserstein distance between mean diagram/measure and the true persistence diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5385a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T10:54:37.383106Z",
     "start_time": "2021-12-04T10:54:29.239344Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the true diagram\n",
    "nb_points = 5000\n",
    "true_set = ApproxPH.sample_annulus(nb_points, r1=0.2, r2=0.5)\n",
    "true_PD = ApproxPH.get_PD(true_set, max_edge_length=0.4, min_persistence=0.01)\n",
    "true_mesr, true_mesr_vis = ApproxPH.diag_to_mesr(true_PD, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3abc4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T11:03:07.021998Z",
     "start_time": "2021-12-04T11:02:52.832006Z"
    }
   },
   "outputs": [],
   "source": [
    "# each time we draw 20 subsets from the true_set\n",
    "# each subset has number of points in nb_sub_points_list\n",
    "# we compute the 2-Wasserstein distance of mean persistence measure & Frechet mean to the true diagram\n",
    "\n",
    "# set parameters\n",
    "nb_subs = 20\n",
    "unit_mass  = 1/nb_samples\n",
    "nb_sub_points_list = [50,100,150,200,250,300,350,400]\n",
    "power_index = 2\n",
    "w_list = []\n",
    "permesr_list = []\n",
    "\n",
    "for nb_sub_points in nb_sub_points_list:\n",
    "    print('number of points in each subset: %d' %(nb_sub_points))\n",
    "    mean_mesr, mean_mesr_vis, wmean = compute_mean(original_set = true_set,\n",
    "                                            nb_subs = nb_subs,\n",
    "                                            nb_sub_points = nb_sub_points,\n",
    "                                            max_edge_length = 0.4,\n",
    "                                            min_persistence = 0.01,\n",
    "                                            scenario = 'both'\n",
    "                                           )\n",
    "    wmean_mesr, wmean_mesr_vis = ApproxPH.diag_to_mesr(wmean, 1)\n",
    "    # compute distance\n",
    "    grid = ApproxPH.mesh_gen()\n",
    "    Mp = ApproxPH.dist_mat(grid, power_index)\n",
    "    permesr_distance = ApproxPH.wass_dist(mean_mesr, true_mesr, Mp)\n",
    "    wmean_distance = ApproxPH.wass_dist(wmean_mesr, true_mesr, Mp)\n",
    "    permesr_list.append(permesr_distance**(1/power_index))\n",
    "    w_list.append(wmean_distance**(1/power_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a7b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T11:08:40.984232Z",
     "start_time": "2021-12-04T11:08:40.862399Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the comparison\n",
    "# plot mean persistence diagram\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(nb_sub_points_list, permesr_list, linestyle='-', color='blue',\\\n",
    "         linewidth=2, label='Mean Persistence Measure')\n",
    "plt.scatter(nb_sub_points_list, permesr_list, s=70, color='red', marker='o')\n",
    "plt.plot(nb_sub_points_list, w_list, linestyle='--', color='green',\\\n",
    "         linewidth=2, label='Frechet Mean')\n",
    "plt.scatter(nb_sub_points_list, w_list, s=70, color='black', marker='P')\n",
    "plt.xlabel('Number of Points')\n",
    "plt.ylabel('2-Wasserstein distance')\n",
    "plt.title('Comparison of Frechet mean\\n and mean persistence measure')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b49b91-3513-4cff-b900-1cc863866d0f",
   "metadata": {},
   "source": [
    "# PH approximation on massive data\n",
    "\n",
    "In this section, we compute the mean persistence measure and Fréchet mean for real large data. We collect two point clouds from the shape repository held by AIM@SHAPE project (http://visionair.ge.imati.cnr.it/ontologies/shapes/). The model IDs are *372-Grayloc_-_Smooth_and_watertight_version* (with 460592 vertices), and *378-knot_with_stars* (with 478704 vertices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b862af-b8bb-4ef4-96d8-c5e6e7682872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read point cloud data\n",
    "\n",
    "import numpy as np\n",
    "from plyfile import *\n",
    "\n",
    "pltdata = PlyData.read('data/grayloc.ply')\n",
    "# pltdata = PlyData.read('data/knot.ply')\n",
    "\n",
    "x = pltdata['vertex']['x']\n",
    "y = pltdata['vertex']['y']\n",
    "z = pltdata['vertex']['z']\n",
    "\n",
    "large_points = ApproxPH.rescale_points(np.array([x,y,z]).T)\n",
    "print(large_points.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4619c-cf53-456e-884f-5de92cbba9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view point cloud\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "ax.scatter(x, y, z, s = 0.05, c=z, alpha=0.7, cmap = 'Blues')\n",
    "ax.view_init(elev=85., azim=30)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed431e59-8376-4a71-8fc1-f9440a9ed9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute persistent homology\n",
    "\n",
    "nb_sub_ratio = 0.02\n",
    "mean_mesr, mean_mesr_vis, wmean = compute_mean(original_set = large_points,\n",
    "                                            nb_subs = 30,\n",
    "                                            nb_sub_points = int(nb_sub_ratio * large_points.shape[0]),\n",
    "                                            max_edge_length = 0.55,\n",
    "                                            min_persistence = 0.07,\n",
    "                                            scenario = 'both'\n",
    "                                           )\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.rcParams.update({'font.family':'Times New Roman', 'font.size':16})\n",
    "plt.rc('text', usetex=True)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(wmean[:,0], wmean[:,1], s=100, marker='o', c='red', alpha=0.8)\n",
    "ax1.plot([0,0.2], [0,0.2], linewidth=0.5)\n",
    "ax1.fill_between([0,0.2], [0,0.2], [0,0], facecolor='green', alpha=0.2)\t\n",
    "ax1.set_xlim((0,0.2))\n",
    "ax1.set_ylim((0,0.5))\n",
    "ax1.set_xticks([0,0.1,0.2])\n",
    "ax1.set_title(\"FM\")\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.imshow(mean_mesr_vis.T, origin='lower', cmap='hot_r', interpolation='bilinear', aspect='auto')\n",
    "L = mean_mesr_vis.shape[0]\n",
    "ax2.set_xlim((0,L/5))\n",
    "ax2.set_ylim((0,L/2))\n",
    "ax2.set_xticks([0,5,10])\n",
    "ax2.set_xticklabels([0.0,0.1,0.2])\n",
    "ax2.set_yticks([0,5,10,15,20,25])\n",
    "ax2.set_yticklabels([0.0,0.1,0.2,0.3,0.4,0.5])\n",
    "ax2.fill_between([0,L/5], [0, L/5], [0,0], facecolor='green', alpha=0.2)\n",
    "ax2.set_title(\"MPM\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154a5e1-536e-4252-9d89-16a41128dad2",
   "metadata": {},
   "source": [
    "# Shape clustering\n",
    "\n",
    "In this section, we show how subsampling methods can be used to apply persistent homolgy to shape clustering tasks. We collect data from Mechenical Component Benchmark (https://mechanical-components.herokuapp.com/). We extract large point sets from classes 'Bearing' and 'Motor' from training sets in MCB_B. The selected point clouds in `data` consist of 30,000 to 250,000 points. It is not feasible to compute persistent homology on these data sets directly. With the help of subsampling, however, we can approximate the persistent homology of each data set and use the 2-Wasserstein distance matrix to apply dimension reduction and clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f6f0906-4770-479e-a074-58344ba1b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "import os\n",
    "\n",
    "bearing_list = [np.load('data/Bearing/'+file) for file in os.listdir('data/Bearing/')]\n",
    "motor_list = [np.load('data/Motor/'+file) for file in os.listdir('data/Motor/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd55a6e-8ebf-4252-9968-bac76ca1b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view point cloud\n",
    "\n",
    "fig = plt.figure(figsize=(8,8)) \n",
    "ax1 = fig.add_subplot(121,projection='3d')\n",
    "points = ApproxPH.rescale_points(bearing_list[0])\n",
    "ax1.scatter3D(points[:,0], points[:,1], points[:,2], s=10, c=points[:,2])\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(122,projection='3d')\n",
    "points = ApproxPH.rescale_points(motor_list[0])\n",
    "ax2.scatter3D(points[:,0], points[:,1], points[:,2], s=10, c=points[:,2])\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cfb0d-555f-497c-9ffb-0fd0439f1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute persistent homology\n",
    "\n",
    "total_list = bearing_list + motor_list\n",
    "print('there are %d point sets' %len(total_list))\n",
    "\n",
    "total_fm_list = []\n",
    "total_mpm_list = []\n",
    "\n",
    "nb_sub_ratio = 0.02\n",
    "for points in total_list:\n",
    "    mean_mesr, mean_mesr_vis, wmean = compute_mean(original_set = ApproxPH.rescale_points(points),\n",
    "                                            nb_subs = 15,\n",
    "                                            nb_sub_points = int(nb_sub_ratio * points.shape[0]),\n",
    "                                            max_edge_length = 0.4,\n",
    "                                            min_persistence = 0.03,\n",
    "                                            scenario = 'both'\n",
    "                                           )\n",
    "    total_fm_list.append(wmean)\n",
    "    total_mpm_list.append(mean_mesr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a566f9-5ea1-4e78-a0dd-ab4161753d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the wasserstein-2 distance matrix for mean persistence measures\n",
    "    \n",
    "N = len(total_list)\n",
    "grid = ApproxPH.mesh_gen()\n",
    "Mp = ApproxPH.dist_mat(grid, 2)\n",
    "mpm_distance_mat = np.zeros((N,N))\n",
    "for i in range(N):\n",
    "    for j in range(i+1,N):\n",
    "        mpm_distance_mat[i,j] = ApproxPH.wass_dist(total_mpm_list[i], total_mpm_list[j], Mp)\n",
    "\n",
    "mpm_distance_mat = mpm_distance_mat + mpm_distance_mat.T\n",
    "\n",
    "# use umap to visualize in 2D atlas\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(random_state=30,n_neighbors=10)\n",
    "embedding = reducer.fit_transform(mpm_distance_mat)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.rcParams.update({'font.family':'Times New Roman', 'font.size':16})\n",
    "plt.rc('text', usetex=True)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(embedding[0:len(bearing_list),0],embedding[0:len(bearing_list),1],s=70,label = 'Bearing')\n",
    "ax1.scatter(embedding[len(bearing_list):,0],embedding[len(bearing_list):,1],s=70,label = 'Motor')\n",
    "ax1.set_title('UMAP for MPM')\n",
    "plt.legend()\n",
    "\n",
    "# use DBSCAN to cluster the points\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=3, min_samples=10).fit(embedding)\n",
    "u_labels = np.unique(clustering.labels_)\n",
    "print(u_labels)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "for label in u_labels:\n",
    "    ax2.scatter(embedding[np.where(clustering.labels_==label)[0],0],\n",
    "                embedding[np.where(clustering.labels_==label)[0],1],s=70)\n",
    "ax2.set_title('DBSCAN for MPM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2511165-ef3a-4150-9d83-ab83eabc0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the wasserstein-2 distance matrix for Frechet mean\n",
    "\n",
    "import gudhi as gd\n",
    "    \n",
    "fm_distance_mat = np.zeros((N,N))\n",
    "for i in range(N):\n",
    "    for j in range(i+1,N):\n",
    "        fm_distance_mat[i,j] = gd.wasserstein.wasserstein_distance(total_fm_list[i], total_fm_list[j], order=2)\n",
    "\n",
    "fm_distance_mat = fm_distance_mat + fm_distance_mat.T\n",
    "\n",
    "# use umap to visualize in 2D atlas\n",
    "\n",
    "reducer = umap.UMAP(random_state=20,n_neighbors=10)\n",
    "embedding = reducer.fit_transform(fm_distance_mat)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.rcParams.update({'font.family':'Times New Roman', 'font.size':16})\n",
    "plt.rc('text', usetex=True)\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(embedding[0:len(bearing_list),0],embedding[0:len(bearing_list),1],s=70,label = 'Bearing')\n",
    "ax1.scatter(embedding[len(bearing_list):,0],embedding[len(bearing_list):,1],s=70,label = 'Motor')\n",
    "ax1.set_title('UMAP for FM')\n",
    "plt.legend()\n",
    "\n",
    "# use DBSCAN to cluster the points\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=2, min_samples=10).fit(embedding)\n",
    "u_labels = np.unique(clustering.labels_)\n",
    "print(u_labels)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "for label in u_labels:\n",
    "    ax2.scatter(embedding[np.where(clustering.labels_==label)[0],0],\n",
    "                embedding[np.where(clustering.labels_==label)[0],1],s=70)\n",
    "ax2.set_title('DBSCAN for FM')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
